{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data/khuangaf/miniconda2/envs/py27/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, LeakyReLU\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout, CuDNNGRU\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 100\n",
    "embedding_dim = 300\n",
    "TIME_STEPS = 100\n",
    "SINGLE_ATTENTION_VECTOR = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.concat([pd.read_csv(\"../input/train.csv\"),pd.read_csv(\"../input/train_es.csv\")] )\n",
    "# train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((319142, 8), (153164, 2))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "word_index = tokenizer.word_index\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index={}\n",
    "f = open( '../input/glove.42B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_features:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File('../input/toxic_token.h5','w') as f:\n",
    "    f.create_dataset(\"X_t\", data = X_t )\n",
    "    f.create_dataset(\"X_te\", data = X_te )\n",
    "    f.create_dataset(\"y\", data = y )\n",
    "    f.create_dataset(\"embedding_matrix\", data=embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "with open('../input/word_index.p', 'wb') as fp:\n",
    "    pickle.dump(word_index, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    global embedding_matrix, embedding_dim\n",
    "    embed_size = embedding_dim\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    \n",
    "    x = Bidirectional(CuDNNGRU(50, return_sequences=True))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Bidirectional(CuDNNGRU(50, return_sequences=False))(x)\n",
    "    x = Dense(40)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.layers[1].set_weights([embedding_matrix])\n",
    "    model.layers[1].trainable = False\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=RMSprop(clipvalue=1, clipnorm=1),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "fname= 'bi-50gru-bi-50gru-300emb-40'\n",
    "file_path=\"weights/\"+fname+\".hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 287227 samples, validate on 31915 samples\n",
      "Epoch 1/10\n",
      "287200/287227 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9813Epoch 00001: val_loss improved from inf to 0.04916, saving model to weights/bi-50gru-bi-50gru-300emb-40.hdf5\n",
      "287227/287227 [==============================] - 401s 1ms/step - loss: 0.0526 - acc: 0.9813 - val_loss: 0.0492 - val_acc: 0.9822\n",
      "Epoch 2/10\n",
      "287168/287227 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9831Epoch 00002: val_loss improved from 0.04916 to 0.04739, saving model to weights/bi-50gru-bi-50gru-300emb-40.hdf5\n",
      "287227/287227 [==============================] - 393s 1ms/step - loss: 0.0460 - acc: 0.9831 - val_loss: 0.0474 - val_acc: 0.9828\n",
      "Epoch 3/10\n",
      "287200/287227 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9841- ETA: 4s - loss: Epoch 00003: val_loss did not improve\n",
      "287227/287227 [==============================] - 394s 1ms/step - loss: 0.0427 - acc: 0.9841 - val_loss: 0.0476 - val_acc: 0.9828\n",
      "Epoch 4/10\n",
      "287200/287227 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9850Epoch 00004: val_loss did not improve\n",
      "287227/287227 [==============================] - 397s 1ms/step - loss: 0.0398 - acc: 0.9850 - val_loss: 0.0481 - val_acc: 0.9821\n",
      "Epoch 5/10\n",
      "287200/287227 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9860Epoch 00005: val_loss did not improve\n",
      "287227/287227 [==============================] - 400s 1ms/step - loss: 0.0370 - acc: 0.9860 - val_loss: 0.0483 - val_acc: 0.9824\n",
      "Epoch 6/10\n",
      "287168/287227 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9865Epoch 00006: val_loss did not improve\n",
      "287227/287227 [==============================] - 398s 1ms/step - loss: 0.0353 - acc: 0.9865 - val_loss: 0.0514 - val_acc: 0.9825\n"
     ]
    }
   ],
   "source": [
    "callbacks_list = [checkpoint, early] #early\n",
    "model.fit(X_t, y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)\n",
    "intermetiate_X = model.predict(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(file_path)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "y_test = model.predict(X_te)\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv(\"output/\"+fname+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
