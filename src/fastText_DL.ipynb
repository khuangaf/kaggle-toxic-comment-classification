{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data/khuangaf/miniconda2/envs/py27/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "import re\n",
    "import string\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, LeakyReLU\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout, CuDNNLSTM, CuDNNGRU\n",
    "from keras.layers import Dense, Embedding, Input, LeakyReLU, merge, Conv2D, Conv1D, PReLU,ELU,Concatenate, Convolution1D\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, GRU, Dropout, CuDNNGRU, Reshape, MaxPool2D,Flatten, Lambda, Activation\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras import regularizers, constraints\n",
    "from keras.optimizers import RMSprop, Adam,Nadam\n",
    "\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File('../input/fasttext_processed_rmnum_extra.h5', 'r') as f:\n",
    "    x_train = f['x_train'].value\n",
    "    y_train = f['y_train'].value\n",
    "    x_test = f['x_test'].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import, division\n",
    "\n",
    "import sys\n",
    "from os.path import dirname\n",
    "# sys.path.append(dirname(dirname(__file__)))\n",
    "from keras import initializers\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Crop(dimension, start, end):\n",
    "    # Crops (or slices) a Tensor on a given dimension from start to end\n",
    "    # example : to crop tensor x[:, :, 5:10]\n",
    "    # call slice(2, 5, 10) as you want to crop on the second dimension\n",
    "    def func(x):\n",
    "        if dimension == 0:\n",
    "            return x[start: end]\n",
    "        if dimension == 1:\n",
    "            return x[:, start: end]\n",
    "        if dimension == 2:\n",
    "            return x[:, :, start: end]\n",
    "        if dimension == 3:\n",
    "            return x[:, :, :, start: end]\n",
    "        if dimension == 4:\n",
    "            return x[:, :, :, :, start: end]\n",
    "    return Lambda(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    global embedding_dim\n",
    "    embed_size = embedding_dim\n",
    "    inp = Input(shape=(maxlen, embedding_dim ))\n",
    "#     x = Embedding(max_features, embed_size)(inp)\n",
    "#     x = Dropout(0.4)(x)\n",
    "    x = SpatialDropout1D(0.2)(inp)\n",
    "    x = Bidirectional(CuDNNGRU(50, return_sequences=True))(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "#     x = Dropout(0.4)(x)\n",
    "#     x = Bidirectional(CuDNNLSTM(50, return_sequences=True))(x)\n",
    "    A = AttentionWeightedAverage(name='attlayer', return_attention=False)(x)\n",
    "#     A = AttentionWithContext()(x)\n",
    "#     x = AttentionWithContext()(x)\n",
    "#     C = Crop(2,-2,-1)(x)\n",
    "#     C = Reshape([-1,])(C)\n",
    "#     print(C)\n",
    "    G = GlobalMaxPool1D()(x)\n",
    "    x = Concatenate()([A,G])\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    x = Dense(50, activation=None)(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "#     model.layers[1].set_weights([embedding_matrix])\n",
    "#     model.layers[1].trainable = False\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(amsgrad=True),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "def train_bagging(X, y, model_func, fold_count, batch_size, num_epoch, patience):\n",
    "    \n",
    "    best_auc_score = -np.inf\n",
    "    best_weights = None\n",
    "    kf = KFold(n_splits=fold_count, random_state=None, shuffle=False)\n",
    "    fold_id = -1\n",
    "    model_list = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        fold_id +=1 \n",
    "        model = model_func()\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        not_improve_count = 0\n",
    "        current_best_auc_score = -np.inf\n",
    "        for e in range(num_epoch):\n",
    "            print(\"Fold {0}, epoch {1}\".format(fold_id, e))\n",
    "            model.fit(X_train,y_train, batch_size=batch_size, verbose=0)\n",
    "            y_pred = model.predict(X_test)\n",
    "            auc = roc_auc_score(y_test, y_pred)\n",
    "            \n",
    "            if auc > current_best_auc_score:\n",
    "                print(\"Current AUC Score improved from {0} to {1}.\".format(current_best_auc_score, auc))\n",
    "                current_best_auc_score = auc\n",
    "                current_best_weights = model.get_weights()\n",
    "                not_improve_count = 0\n",
    "            else:\n",
    "                print(\"Current AUC Score did not improved. {}\".format(auc))\n",
    "                not_improve_count += 1\n",
    "                if not_improve_count >= patience:\n",
    "                    model.set_weights(current_best_weights)\n",
    "                    model_list.append(model)\n",
    "                    print (\"Model appended.\")\n",
    "                    break\n",
    "        if current_best_auc_score > best_auc_score:\n",
    "            print(\"Best AUC Score improved from {0} to {1}.\".format(best_auc_score, current_best_auc_score))\n",
    "            best_weights = current_best_weights\n",
    "            best_auc_score = current_best_auc_score\n",
    "        else:\n",
    "            print(\"Best AUC Score did not improved. {0}\".format(current_best_auc_score))\n",
    "        \n",
    "            \n",
    "#     model.set_weights(best_weights)\n",
    "    return model_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_model_func = lambda : get_model()\n",
    "fname='sp_bigru_relu_ft100_d1_atten_global_amsgrad_bag_rmnum_extra'\n",
    "batch_size= 512\n",
    "epochs = 50\n",
    "\n",
    "patience=6\n",
    "model_list = train_bagging(x_train, y_train, get_model_func, fold_count= 10, batch_size=batch_size, num_epoch= epochs, patience=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, model in enumerate(model_list):\n",
    "    if index == 0: \n",
    "        y_pred = model.predict(x_test, verbose=1, batch_size=batch_size)\n",
    "    else:\n",
    "        y_pred += model.predict(x_test, verbose=1,batch_size=batch_size)\n",
    "    \n",
    "y_pred = y_pred / len(model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 574455 samples, validate on 63829 samples\n",
      "Epoch 1/50\n",
      "574455/574455 [==============================] - 173s 302us/step - loss: 0.0879 - acc: 0.9713 - val_loss: 0.0548 - val_acc: 0.9801\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05480, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 2/50\n",
      "574455/574455 [==============================] - 174s 302us/step - loss: 0.0540 - acc: 0.9806 - val_loss: 0.0511 - val_acc: 0.9812\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05480 to 0.05115, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 3/50\n",
      "574455/574455 [==============================] - 170s 296us/step - loss: 0.0500 - acc: 0.9815 - val_loss: 0.0487 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05115 to 0.04868, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 4/50\n",
      "574455/574455 [==============================] - 183s 318us/step - loss: 0.0479 - acc: 0.9822 - val_loss: 0.0470 - val_acc: 0.9825\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04868 to 0.04704, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 5/50\n",
      "574455/574455 [==============================] - 270s 469us/step - loss: 0.0464 - acc: 0.9826 - val_loss: 0.0463 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04704 to 0.04629, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 6/50\n",
      "574455/574455 [==============================] - 210s 365us/step - loss: 0.0451 - acc: 0.9829 - val_loss: 0.0451 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.04629 to 0.04507, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 7/50\n",
      "574455/574455 [==============================] - 189s 329us/step - loss: 0.0440 - acc: 0.9832 - val_loss: 0.0438 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.04507 to 0.04380, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 8/50\n",
      "574455/574455 [==============================] - 180s 313us/step - loss: 0.0431 - acc: 0.9835 - val_loss: 0.0435 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.04380 to 0.04345, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 9/50\n",
      "574455/574455 [==============================] - 176s 306us/step - loss: 0.0422 - acc: 0.9838 - val_loss: 0.0427 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.04345 to 0.04266, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 10/50\n",
      "574455/574455 [==============================] - 187s 325us/step - loss: 0.0414 - acc: 0.9840 - val_loss: 0.0424 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.04266 to 0.04243, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 11/50\n",
      "574455/574455 [==============================] - 208s 362us/step - loss: 0.0408 - acc: 0.9842 - val_loss: 0.0422 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.04243 to 0.04215, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 12/50\n",
      "574455/574455 [==============================] - 199s 347us/step - loss: 0.0404 - acc: 0.9844 - val_loss: 0.0414 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.04215 to 0.04137, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 13/50\n",
      "574455/574455 [==============================] - 200s 349us/step - loss: 0.0397 - acc: 0.9845 - val_loss: 0.0411 - val_acc: 0.9845\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.04137 to 0.04109, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 14/50\n",
      "574455/574455 [==============================] - 189s 330us/step - loss: 0.0392 - acc: 0.9848 - val_loss: 0.0406 - val_acc: 0.9845\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.04109 to 0.04063, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 15/50\n",
      "574455/574455 [==============================] - 162s 281us/step - loss: 0.0387 - acc: 0.9849 - val_loss: 0.0401 - val_acc: 0.9849\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.04063 to 0.04012, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 16/50\n",
      "574455/574455 [==============================] - 154s 268us/step - loss: 0.0381 - acc: 0.9850 - val_loss: 0.0395 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.04012 to 0.03946, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 17/50\n",
      "574455/574455 [==============================] - 153s 266us/step - loss: 0.0378 - acc: 0.9853 - val_loss: 0.0392 - val_acc: 0.9851\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.03946 to 0.03924, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 18/50\n",
      "574455/574455 [==============================] - 154s 268us/step - loss: 0.0372 - acc: 0.9854 - val_loss: 0.0394 - val_acc: 0.9852\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/50\n",
      "574455/574455 [==============================] - 157s 274us/step - loss: 0.0370 - acc: 0.9854 - val_loss: 0.0389 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.03924 to 0.03887, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 20/50\n",
      "574455/574455 [==============================] - 157s 273us/step - loss: 0.0366 - acc: 0.9856 - val_loss: 0.0384 - val_acc: 0.9854\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.03887 to 0.03841, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 21/50\n",
      "574455/574455 [==============================] - 158s 275us/step - loss: 0.0364 - acc: 0.9857 - val_loss: 0.0396 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/50\n",
      "574455/574455 [==============================] - 166s 289us/step - loss: 0.0359 - acc: 0.9859 - val_loss: 0.0382 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.03841 to 0.03824, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 23/50\n",
      "574455/574455 [==============================] - 169s 295us/step - loss: 0.0356 - acc: 0.9860 - val_loss: 0.0378 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.03824 to 0.03775, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 24/50\n",
      "574455/574455 [==============================] - 162s 281us/step - loss: 0.0354 - acc: 0.9861 - val_loss: 0.0377 - val_acc: 0.9859\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.03775 to 0.03773, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 25/50\n",
      "574455/574455 [==============================] - 161s 280us/step - loss: 0.0350 - acc: 0.9862 - val_loss: 0.0380 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/50\n",
      "574455/574455 [==============================] - 161s 280us/step - loss: 0.0347 - acc: 0.9863 - val_loss: 0.0372 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.03773 to 0.03721, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 27/50\n",
      "574455/574455 [==============================] - 160s 279us/step - loss: 0.0344 - acc: 0.9864 - val_loss: 0.0376 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/50\n",
      "574455/574455 [==============================] - 163s 284us/step - loss: 0.0343 - acc: 0.9864 - val_loss: 0.0369 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.03721 to 0.03694, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 29/50\n",
      "574455/574455 [==============================] - 158s 274us/step - loss: 0.0340 - acc: 0.9865 - val_loss: 0.0370 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/50\n",
      "574455/574455 [==============================] - 155s 270us/step - loss: 0.0339 - acc: 0.9866 - val_loss: 0.0364 - val_acc: 0.9864\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.03694 to 0.03643, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 31/50\n",
      "574455/574455 [==============================] - 156s 272us/step - loss: 0.0337 - acc: 0.9866 - val_loss: 0.0364 - val_acc: 0.9864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00031: val_loss improved from 0.03643 to 0.03643, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 32/50\n",
      "574455/574455 [==============================] - 156s 271us/step - loss: 0.0335 - acc: 0.9867 - val_loss: 0.0362 - val_acc: 0.9865\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.03643 to 0.03618, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 33/50\n",
      "574455/574455 [==============================] - 156s 271us/step - loss: 0.0332 - acc: 0.9869 - val_loss: 0.0371 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/50\n",
      "574455/574455 [==============================] - 155s 269us/step - loss: 0.0330 - acc: 0.9869 - val_loss: 0.0363 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/50\n",
      "574455/574455 [==============================] - 148s 258us/step - loss: 0.0329 - acc: 0.9870 - val_loss: 0.0367 - val_acc: 0.9866\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/50\n",
      "574455/574455 [==============================] - 153s 267us/step - loss: 0.0326 - acc: 0.9871 - val_loss: 0.0360 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.03618 to 0.03600, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 37/50\n",
      "574455/574455 [==============================] - 173s 300us/step - loss: 0.0324 - acc: 0.9871 - val_loss: 0.0359 - val_acc: 0.9868\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.03600 to 0.03589, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 38/50\n",
      "574455/574455 [==============================] - 156s 272us/step - loss: 0.0323 - acc: 0.9872 - val_loss: 0.0358 - val_acc: 0.9868\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.03589 to 0.03582, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 39/50\n",
      "574455/574455 [==============================] - 171s 297us/step - loss: 0.0322 - acc: 0.9872 - val_loss: 0.0355 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.03582 to 0.03551, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 40/50\n",
      "574455/574455 [==============================] - 166s 289us/step - loss: 0.0319 - acc: 0.9873 - val_loss: 0.0356 - val_acc: 0.9869\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/50\n",
      "574455/574455 [==============================] - 159s 276us/step - loss: 0.0318 - acc: 0.9874 - val_loss: 0.0353 - val_acc: 0.9869\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.03551 to 0.03534, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 42/50\n",
      "574455/574455 [==============================] - 161s 281us/step - loss: 0.0317 - acc: 0.9874 - val_loss: 0.0363 - val_acc: 0.9862\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/50\n",
      "574455/574455 [==============================] - 149s 259us/step - loss: 0.0316 - acc: 0.9874 - val_loss: 0.0352 - val_acc: 0.9869\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.03534 to 0.03515, saving model to weights/sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra.hdf5\n",
      "Epoch 44/50\n",
      "485376/574455 [========================>.....] - ETA: 21s - loss: 0.0311 - acc: 0.9876"
     ]
    }
   ],
   "source": [
    "fname='sp_bigru_relu_ft100_d1_atten_global_amsgrad_rmnum_extra'\n",
    "filepath='weights/'+fname+'.hdf5'\n",
    "batch_size= 1024\n",
    "epochs = 50\n",
    "model = get_model()\n",
    "early = EarlyStopping(monitor='val_loss', patience=6,mode='auto')\n",
    "checkpoints = ModelCheckpoint(filepath=filepath, save_best_only=True,verbose=1)\n",
    "callbacks = [early, checkpoints]\n",
    "model.fit(x_train, y_train, epochs =epochs, validation_split=0.1, batch_size=batch_size, callbacks=callbacks)\n",
    "# model.load_weights(filepath)\n",
    "# y_pred = model.predict(x_test, verbose=1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ids = test_data[\"id\"].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "\n",
    "CLASSES = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "test_predicts = pd.DataFrame(data=y_pred, columns=CLASSES)\n",
    "test_predicts[\"id\"] = test_ids\n",
    "test_predicts = test_predicts[[\"id\"] + CLASSES]\n",
    "submit_path = os.path.join(\"output\", fname+\".csv\")\n",
    "test_predicts.to_csv(submit_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
